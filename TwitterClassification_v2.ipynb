{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Zi-MaTRZZQ3q",
        "JMGuxTOkarOY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSCQn96RT3mU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e03eaa6-3bda-4fc4-d0cd-93ce37dfc92b"
      },
      "source": [
        "!wget https://github.com/luisgasco/ntic_master_datos/raw/main/datasets/datos_twitter_master.tsv\n",
        "!pip install emoji_extractor\n",
        "!pip install emoji\n",
        "import sys  \n",
        "!{sys.executable} -m pip install contractions\n",
        "!pip install spacy==3.2.1\n",
        "!python -m spacy download en_core_web_sm\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# Download emoji sentiment\n",
        "!wget https://www.clarin.si/repository/xmlui/handle/11356/1048/allzip\n",
        "!unzip allzip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-08 17:09:25--  https://github.com/luisgasco/ntic_master_datos/raw/main/datasets/datos_twitter_master.tsv\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/luisgasco/ntic_master_datos/main/datasets/datos_twitter_master.tsv [following]\n",
            "--2023-05-08 17:09:26--  https://raw.githubusercontent.com/luisgasco/ntic_master_datos/main/datasets/datos_twitter_master.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 113587 (111K) [text/plain]\n",
            "Saving to: ‘datos_twitter_master.tsv’\n",
            "\n",
            "datos_twitter_maste 100%[===================>] 110.92K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-05-08 17:09:26 (6.76 MB/s) - ‘datos_twitter_master.tsv’ saved [113587/113587]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji_extractor\n",
            "  Downloading emoji_extractor-2.0.0.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji_extractor\n",
            "  Building wheel for emoji_extractor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji_extractor: filename=emoji_extractor-2.0.0-py3-none-any.whl size=64178 sha256=86bc4b6d1337738c810d6b913e9e50a656fe91dd18075c08d9e335b581f7f5b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/89/95/33d1b161e1daa8fb2dec089e887d3620d3a318fdb0798e9f58\n",
            "Successfully built emoji_extractor\n",
            "Installing collected packages: emoji_extractor\n",
            "Successfully installed emoji_extractor-2.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=dd89cd31147404be6a81fed725e6afe1b31a18d5ffabf0e602b4a5bc522d96a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/3d/88/51a592b9ad17e7899126563698b4e3961983ebe85747228ba6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==3.2.1\n",
            "  Downloading spacy-3.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (2.0.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (0.7.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (3.0.8)\n",
            "Collecting wasabi<1.1.0,>=0.8.1\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (0.10.1)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (3.3.0)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (2.27.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (1.22.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (2.0.7)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (23.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (4.65.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (2.4.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (1.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (3.1.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy==3.2.1) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy==3.2.1) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1) (2.0.12)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy==3.2.1) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.2.1) (2.1.2)\n",
            "Installing collected packages: wasabi, typer, pydantic, thinc, spacy\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.1\n",
            "    Uninstalling wasabi-1.1.1:\n",
            "      Successfully uninstalled wasabi-1.1.1\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.7.0\n",
            "    Uninstalling typer-0.7.0:\n",
            "      Successfully uninstalled typer-0.7.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.7\n",
            "    Uninstalling pydantic-1.10.7:\n",
            "      Successfully uninstalled pydantic-1.10.7\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.9\n",
            "    Uninstalling thinc-8.1.9:\n",
            "      Successfully uninstalled thinc-8.1.9\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 6.0.4 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-1.8.2 spacy-3.2.1 thinc-8.0.17 typer-0.4.2 wasabi-0.10.1\n",
            "2023-05-08 17:10:47.017640: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-08 17:10:51.246832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl#egg=en_core_web_sm==3.2.0 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (67.7.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.22.4)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.65.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.12)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (23.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.5.0\n",
            "    Uninstalling en-core-web-sm-3.5.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.5.0\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-08 17:11:04--  https://www.clarin.si/repository/xmlui/handle/11356/1048/allzip\n",
            "Resolving www.clarin.si (www.clarin.si)... 95.87.154.205\n",
            "Connecting to www.clarin.si (www.clarin.si)|95.87.154.205|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘allzip’\n",
            "\n",
            "allzip                  [   <=>              ]  94.61K  79.0KB/s    in 1.2s    \n",
            "\n",
            "2023-05-08 17:11:09 (79.0 KB/s) - ‘allzip’ saved [96878]\n",
            "\n",
            "Archive:  allzip\n",
            "  inflating: ESR_v1.0_format.txt     \n",
            "  inflating: Emoji_Sentiment_Data_v1.0.csv  \n",
            "  inflating: Emojitracker_20150604.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL_KPNOkORKV"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOe_gsA_x9Eh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import contractions\n",
        "import re\n",
        "from emoji_extractor.extract import Extractor\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import en_core_web_sm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8WynRVxPU0z"
      },
      "source": [
        "# Funciones que se utilizarán"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpGYyDY5x73m"
      },
      "source": [
        "# Preparar diccionario de emojis\n",
        "def load_emoji_sentiment(path):\n",
        "  # Cargamos el csv de emoji_sentiment\n",
        "  emoji_sent_df = pd.read_csv(path,sep=\",\")\n",
        "  # Calculamos los scores dividiendo el número de emojis negativos y entre el total\n",
        "  emoji_sent_df[\"Negative\"] = emoji_sent_df[\"Negative\"]/emoji_sent_df[\"Occurrences\"]\n",
        "  emoji_sent_df[\"Neutral\"] = emoji_sent_df[\"Neutral\"]/emoji_sent_df[\"Occurrences\"]\n",
        "  emoji_sent_df[\"Positive\"] = emoji_sent_df[\"Positive\"]/emoji_sent_df[\"Occurrences\"]\n",
        "  # Transformamos a dict\n",
        "  emoji_sent_df = emoji_sent_df.set_index('Emoji')\n",
        "  emoji_dict = emoji_sent_df.to_dict(orient=\"index\")\n",
        "  return emoji_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xtfAvxiPmRM"
      },
      "source": [
        "# Reemplazar contracciones y slang en inglés usando la librería \"contractions\" https://github.com/kootenpv/contractions\n",
        "def replace_contraction(text):\n",
        "    expanded_words = []\n",
        "    # Divide el texto\n",
        "    for t in text.split():\n",
        "        # Aplica la función fix en cada sección o token del texto buscando contracciones y slang\n",
        "        expanded_words.append(contractions.fix(t, slang = True))\n",
        "    expanded_text = ' '.join(expanded_words) \n",
        "    return expanded_text\n",
        "\n",
        "# Hay un tokenizador guay para twitter https://github.com/jaredks/tweetokenize\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7fGZmT1QWxi"
      },
      "source": [
        "# Función para extraer emojis del texto en formato lista\n",
        "def extract_emojis(text):\n",
        "  extract = Extractor()\n",
        "  emojis = extract.count_emoji(text, check_first=False)\n",
        "  emojis_list = [key for key, _ in emojis.most_common()]\n",
        "  return emojis_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfDZavZWRrr2"
      },
      "source": [
        "# Calcula el sentimiento de los emojis de una lista utilizando el diccionario\n",
        "# de emoji sentiment score generado previamente con la función load_emoji_sentiment()\n",
        "# Se puede extraer el valor de positividad de los emojis con la option \"positive\"\n",
        "# Se puede extraer el valor de neutralidad de los emojis con la option \"neutral\"\"  \n",
        "# Se puede extraer el valor de e negatividad de los emojis con la option \"negative\"\"  \n",
        "\n",
        "def get_emoji_sentiment(lista, option = \"positive\"):\n",
        "  output = 0\n",
        "  for emoji in lista:\n",
        "    try:\n",
        "      if option == \"positive\":\n",
        "        output = output + emoji_sent_dict[emoji][\"Positive\"]\n",
        "      elif option ==\"negative\":\n",
        "        output = output + emoji_sent_dict[emoji][\"Negative\"]\n",
        "      elif option ==\"neutral\":\n",
        "        output = output + emoji_sent_dict[emoji][\"Neutral\"]\n",
        "    except Exception as e: \n",
        "      continue\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iidMorvfQX8g"
      },
      "source": [
        "# Eliminar los emojis de un texto. Esto es útil porque una vez extraido los emojis\n",
        "# puede interesarnos tener un texto sin presencia de emojis para mejor análisis.\n",
        "def clean_emoji(text):\n",
        "    # Poner todos los comandos de http://www.unicode.org/Public/emoji/1.0/emoji-data.txt\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F300-\\U0001F6FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u'\\u2600-\\u26FF\\u2700-\\u27BF'\n",
        "        u'\\u2934' u'\\u2935' u'\\u2B05' u'\\u2B06' u'\\u2B07' u'\\u2B1B' u'\\u2B1C' \n",
        "        u'\\u2B50' u'\\u2B55' u'\\u3030' u'\\u303D' u'\\u3297' u'\\u3299' u'\\u00A9'\n",
        "        u'\\u00AE' u'\\u203C' u'\\u2049' u'\\u2122' u'\\u2139' u'\\u2194-\\u2199' \n",
        "        u'\\u21A9' u'\\u21AA' u'\\u231A' u'\\u231B' u'\\u2328' u'\\u23CF'\n",
        "        u'\\u23E9-\\u23F3' u'\\u23F8' u'\\u23F9' u'\\u23FA' u'\\u24C2' u'\\u25AA'\n",
        "        u'\\u25AB' u'\\u25B6' u'\\u25C0' u'\\u25FB' u'\\u25FD' u'\\u25FC' u'\\u25FE'\n",
        "        ']+', flags=re.UNICODE)\n",
        "    string2 = re.sub(emoji_pattern,r' ',text)\n",
        "    return string2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFvpD7AoQ_BI"
      },
      "source": [
        "# Tokenizar los tweets con el tokenizador \"TweetTokenizer\" de NLTK\n",
        "def tokenize(texto):\n",
        "  tweet_tokenizer = TweetTokenizer()\n",
        "  tokens_list = tweet_tokenizer.tokenize(texto)\n",
        "  return tokens_list\n",
        "\n",
        "# Quitar stop words de una lista de tokens\n",
        "def quitar_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "    return filtered_sentence\n",
        "\n",
        "\n",
        "# Eliminar signos de puntuación de una lista de tokens\n",
        "# (nos quedamos sólo lo alfanumérico en este caso)\n",
        "def quitar_puntuacion(tokens):\n",
        "    words=[word for word in tokens if word.isalnum()]\n",
        "    return words\n",
        "\n",
        "\n",
        "# Lemmatization de los tokens. Devuelve una string entera para hacer la tokenización\n",
        "# con NLTK\n",
        "nlp = en_core_web_sm.load(disable=['parser', 'ner'])\n",
        "def lematizar(tokens):\n",
        "    sentence = \" \".join(tokens)\n",
        "    mytokens = nlp(sentence)\n",
        "    # Lematizamos los tokens y los convertimos  a minusculas\n",
        "    mytokens = [ word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "    # Extraemos el text en una string\n",
        "    return \" \".join(mytokens)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkJhe7nYTRVG"
      },
      "source": [
        "# Cargar y preparar los datos\n",
        "En primer lugar cargamos los datos que vamos a utilizar en este notebook (emoji_sentiment_data y datos de twitter)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ID1bUphyILE"
      },
      "source": [
        "emoji_sent_dict = load_emoji_sentiment(\"Emoji_Sentiment_Data_v1.0.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOvvCLWJT1Ll"
      },
      "source": [
        "Podemos textear que hemos obtenido un diccionario cuyas claves son los emojis presentes dentro de emoji sentiment score. Cada emoji tiene un score de negatividad, neutralidad, positividad y otros campos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok9ozYguVFMd",
        "outputId": "77ae65d6-ccfc-485b-962a-df21e847a099"
      },
      "source": [
        "emoji_sent_dict[\"😭\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Unicode codepoint': '0x1f62d',\n",
              " 'Occurrences': 5526,\n",
              " 'Position': 0.803351976,\n",
              " 'Negative': 0.4364820846905538,\n",
              " 'Neutral': 0.22041259500542887,\n",
              " 'Positive': 0.34310532030401736,\n",
              " 'Unicode name': 'LOUDLY CRYING FACE',\n",
              " 'Unicode block': 'Emoticons'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTQVDcH7VpxO"
      },
      "source": [
        "Cargamos el fichero .tsv con los datos de Twitter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "9Am6BD7ET-NB",
        "outputId": "95b737d5-cfa5-4bf3-d936-1f9fa9621cb1"
      },
      "source": [
        "dataset = pd.read_csv(\"datos_twitter_master.tsv\", sep=\"\\t\")\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             tweet_text  molestia\n",
              "0     Noise harassment is a sensation pain based tor...         1\n",
              "1     It's 4.30am and we still haven't slept because...         1\n",
              "2     These birds acting like I can't grab my chains...         1\n",
              "3     Why do people leave the annoying tap-tap keybo...         1\n",
              "4     Please would you keep the noise down? We're re...         1\n",
              "...                                                 ...       ...\n",
              "1015  I come alive when I hear your voice it's a bea...         0\n",
              "1016  I'm currently in Ripon, the noise of the thund...         0\n",
              "1017  Sitting down the weir and the noise of the wat...         0\n",
              "1018  The sound of a beer can being cracked open is ...         0\n",
              "1019  I feel so lucky that today is about the quiete...         0\n",
              "\n",
              "[1020 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c29160d3-4ff9-4205-9367-775880b35244\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>molestia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Noise harassment is a sensation pain based tor...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It's 4.30am and we still haven't slept because...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>These birds acting like I can't grab my chains...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why do people leave the annoying tap-tap keybo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Please would you keep the noise down? We're re...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>I come alive when I hear your voice it's a bea...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>I'm currently in Ripon, the noise of the thund...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>Sitting down the weir and the noise of the wat...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>The sound of a beer can being cracked open is ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>I feel so lucky that today is about the quiete...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1020 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c29160d3-4ff9-4205-9367-775880b35244')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c29160d3-4ff9-4205-9367-775880b35244 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c29160d3-4ff9-4205-9367-775880b35244');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXS6rO_8X_gF"
      },
      "source": [
        "# Análisis preliminar "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcxAkTt6RG2U"
      },
      "source": [
        "# Análisis exploratorio de los datos (EDA)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5ZZJJilSUbe"
      },
      "source": [
        "En este apartado pretendemos realizar un análisis de los datos previo a la normalización de los mismos. Este análisis nos va a permitir extraer información relevante del dataset, así como posibles inconvenientes que serán solucionados llegado el caso.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGFFWS9Q5uPg"
      },
      "source": [
        "* **Número de documentos y columnas:**\n",
        "\n",
        "Comenzamos mostrando el número de documentos, o lo que es lo mismo, el número de filas del data frame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cMr_JLnXyLv",
        "outputId": "db6e7ff2-3d70-4135-923a-d6aec174c1bb"
      },
      "source": [
        "print(\"Tenemos un conjunto de {} documentos\".format(len(dataset)))\n",
        "print(\"El dataframe tiene {} columnas\".format(dataset.shape[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tenemos un conjunto de 1020 documentos\n",
            "El dataframe tiene 2 columnas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Número de documentos duplicados:**"
      ],
      "metadata": {
        "id": "TOwki0MDKR9c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUMrCNQLWnER"
      },
      "source": [
        "Despues, comprobamos y eliminamos las filas con algún valor vacío (NA) y quitaremos los duplicados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlTJyjagXNIK",
        "outputId": "c3dd5b3c-edac-4e4e-a948-3a5d46efa4a3"
      },
      "source": [
        "print(\"Existen {} noticias duplicadas\".format(np.sum(dataset.duplicated(subset=[\"tweet_text\"]))))\n",
        "# Quitaremos esos duplicados\n",
        "dataset = dataset.drop_duplicates()\n",
        "print(\"Despues de quitar duplicados tenemos un conjunto de {} noticias\".format(dataset.shape[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existen 1 noticias duplicadas\n",
            "Despues de quitar duplicados tenemos un conjunto de 1019 noticias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR2fqxu6WrGo"
      },
      "source": [
        "Comprobaramos que no hayan quedado Nulls en ningunas de las dos columnas del dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEvobTAsXmMy",
        "outputId": "36269153-a18f-4f0c-97d7-22d1ef535077"
      },
      "source": [
        "print(\"Hay {} valores vacíos en las noticias y {} valores vacíos en las etiquetas en los datos\".format(np.sum(dataset.isnull())[0],\n",
        "                                                                                                        np.sum(dataset.isnull())[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hay 0 valores vacíos en las noticias y 0 valores vacíos en las etiquetas en los datos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Número de documentos por cada clase:**\n",
        "\n",
        "Contamos el número de elementos de cada clase. Vemos que en la columna \"molestia\" nos encontramos las etiquetas del dataset. En este caso nos encontramos dos tipos de documentos (tweets):\n",
        "\n",
        "- \"Molestia = 1\": Tweets con la palabra ruido que hacen referencia a molestias sufridas por ruido acústico proveniente de distintas fuentes (coches, vecinos, mascotas,...)\n",
        "- \"Molestia = 0\": Tweets que contienen la palabra ruido perso no expresan una molestia sufrida por el usuario que lo escribió (otras acpciones de ruido, noticias que hablan sobre ruido o uso de ruido como algo positivo) "
      ],
      "metadata": {
        "id": "h24AMcCZKnbw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY8ksiYvZvqt"
      },
      "source": [
        "Comprobemos la distribución de las clases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ead7ae00-41f9-438e-c06a-d300cca7f57f",
        "id": "U1AYyDfTZvqu"
      },
      "source": [
        "dataset[\"molestia\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    510\n",
              "0    509\n",
              "Name: molestia, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvLsb-_gaFyH"
      },
      "source": [
        "¡¡Tenemos un dataset balanceado!! Esto nos evitará problemas en el entrenamiento de los modelos😀. \n",
        "\n",
        "Disponemos 509 noticias verdaderas (valor 0) y 29571 noticias falsas (valor 1).\n",
        "\n",
        "Vamos a dibujar un histograma con las clases así practicamos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "imbF43WHaFOz",
        "outputId": "db544d44-fd1a-487c-ec58-10adcf3e8b38"
      },
      "source": [
        "ax, fig = plt.subplots()\n",
        "etiquetas = dataset.molestia.value_counts()\n",
        "etiquetas.plot(kind= 'bar', color= [\"blue\", \"orange\"])\n",
        "plt.title('Bar chart')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ors90Yon5aJf"
      },
      "source": [
        "* **Distribución de la longitud de los tweet en caracteres:**\n",
        "\n",
        "Para seguir con el análisis exploratorio, vamos a hacer un cálculo típico: la longitud de cada uno de los textos de los documentos para despues dibujar su histograma. \n",
        "\n",
        "Comenzamos creando las columnas que van a almacenar las longitud en caracteres y en tokens de los documentos del corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVjO14rKXWXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e97818-3395-43d2-cd73-508cfe334ef1"
      },
      "source": [
        "dataset[\"char_len\"] = dataset[\"tweet_text\"].apply(lambda x: len(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-6bd16aaca85c>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"char_len\"] = dataset[\"tweet_text\"].apply(lambda x: len(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librerías matplotlib y seaborn:\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig = plt.figure(figsize=(14,12))\n",
        "sns.set_style(\"darkgrid\")\n",
        "# añadimos series para cada categoría (eligiendo la seríe de char_len\n",
        "plt1 = sns.distplot(dataset[dataset[\"molestia\"]==0].char_len, hist=True, label=\"no_molestia\")\n",
        "plt2 = sns.distplot(dataset[dataset[\"molestia\"]==1].char_len, hist=True, label=\"molestia\")\n",
        "fig.legend(labels=['no molestia','molestia'], loc = 5)\n",
        "\n",
        "\n",
        "# Definimos el título de los ejes:\n",
        "plt.xlabel('Caracteres', fontsize=16)\n",
        "plt.ylabel('Densidad', fontsize=16)\n",
        "\n",
        "# Finalmente mostramos el gráfico:\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9Qh8tMrcLSmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKMCoo9-XnmX"
      },
      "source": [
        "En la figura se ve que no existen diferencias significativas entre las dos clases. Quizá los tweets en los que el usuario se queja sobre el ruido (molestia ==1)tienen una tendencia a ser más cortos, pero no se observa nada destacable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YURv-wywYDpq"
      },
      "source": [
        "# Transformación \n",
        "\n",
        "Como hemos visto, está dividido en dos pasos Normalización o Preprocesado y Transformación\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG9xxARQYJ_8"
      },
      "source": [
        "\n",
        "## Normalización\n",
        "Vamos a proceder a normalizar los datos. Para ello vamos a utilizar las funciones anteriormente definidas:\n",
        "\n",
        "- Por una parte vamos a extraer los emojis de los tweets, los vamos a guardar en una lista dentro de una nueva columna del dataframe y por último calcularemos un valor de sentimiento de emojis de positividad, negatividad y neutralidad.\n",
        "\n",
        "- Preprocesar los textos:\n",
        "    - Primero expanderemos las contracciones de los tweets\n",
        "    - Despues quitaremos los emojis, ya que antes habremos calculado los scores necesarios.\n",
        "    - Tokenizaremos\n",
        "    - Quitaremos stop words\n",
        "    - Quitaremos puntuación\n",
        "    - Lematizaremos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lematizar(tokens):\n",
        "    sentence = \" \".join(tokens)\n",
        "    mytokens = nlp(sentence)\n",
        "    # Lematizamos los tokens y los convertimos  a minusculas\n",
        "    mytokens = [ word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "    # Extraemos el text en una string\n",
        "    return \" \".join(mytokens)"
      ],
      "metadata": {
        "id": "epXWuKOdYoBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi-MaTRZZQ3q"
      },
      "source": [
        "### Emojis\n",
        "En primer luigar vamos a trabajar con los emojis. \n",
        "\n",
        "Vamos a extraerlos con una función lambda aplicando la función extract_emojis() definida anteriormente en el dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ7t-9ZQYX6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27887937-7ce7-4441-b7da-6ccb66b2d837"
      },
      "source": [
        "dataset[\"emoji_list\"] = dataset[\"tweet_text\"].apply(lambda x: extract_emojis(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-bb213c0cc61f>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"emoji_list\"] = dataset[\"tweet_text\"].apply(lambda x: extract_emojis(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od_3GHZsZwaz"
      },
      "source": [
        "Vemos que nos ha guardado los emojis en la columna \"emoji_list\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxW7SeDIZwAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3445e500-afc0-46cc-e88f-80266e487d89"
      },
      "source": [
        "dataset[\"emoji_list\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        []\n",
              "1        []\n",
              "2        []\n",
              "3        []\n",
              "4       [🎻]\n",
              "       ... \n",
              "1015     []\n",
              "1016     []\n",
              "1017     []\n",
              "1018     []\n",
              "1019     []\n",
              "Name: emoji_list, Length: 1019, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akaI1mmAaHbl"
      },
      "source": [
        "A continuación, se calcula un score de sentimiento a los emojis asociados a cada tweet. Si no hay emojis, estos scores serán cero.\n",
        "Para calcular esto lo haremos de nuevo con funciones lambda aplicando la función get_emoji_sentiment() anteriormente generada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzdd6JrlaYeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d450701e-b683-4c55-aea4-c051c3385aab"
      },
      "source": [
        "dataset[\"sent_emoji_pos\"] = dataset[\"emoji_list\"].apply(lambda x: get_emoji_sentiment(x, \"positive\"))\n",
        "dataset[\"sent_emoji_neu\"] = dataset[\"emoji_list\"].apply(lambda x: get_emoji_sentiment(x, \"neutral\"))\n",
        "dataset[\"sent_emoji_neg\"] = dataset[\"emoji_list\"].apply(lambda x: get_emoji_sentiment(x, \"negative\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-bae707eba02b>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"sent_emoji_pos\"] = dataset[\"emoji_list\"].apply(lambda x: get_emoji_sentiment(x, \"positive\"))\n",
            "<ipython-input-27-bae707eba02b>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"sent_emoji_neu\"] = dataset[\"emoji_list\"].apply(lambda x: get_emoji_sentiment(x, \"neutral\"))\n",
            "<ipython-input-27-bae707eba02b>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"sent_emoji_neg\"] = dataset[\"emoji_list\"].apply(lambda x: get_emoji_sentiment(x, \"negative\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI2Ekz7SZulT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897e6ee0-4bbd-442b-b988-cb9538a35a7f"
      },
      "source": [
        "dataset[\"sent_emoji_pos\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1019.000000\n",
              "mean        0.063655\n",
              "std         0.194156\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         0.000000\n",
              "max         2.175897\n",
              "Name: sent_emoji_pos, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMGuxTOkarOY"
      },
      "source": [
        "### Preprocesar textos\n",
        "Vamos a realizar los preprocesados indicados antes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPfOeLYua43J"
      },
      "source": [
        " En primer lugar expandimos las contracciones. Además, despues del proceso de extracción de emojis, los quitaremos de nuestros textos porque no nos serán útiles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dru7LpIBZ31S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78134c15-9902-4c1c-8c02-c4a54186b60a"
      },
      "source": [
        "# Reemplazar contracciones\n",
        "dataset[\"tweet_text_processed\"] = dataset[\"tweet_text\"].apply(lambda x: replace_contraction(x))\n",
        "# Quitar emojis de los textos\n",
        "dataset[\"tweet_text_processed\"] = dataset[\"tweet_text_processed\"].apply(lambda x: clean_emoji(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-15121a3b8744>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"tweet_text_processed\"] = dataset[\"tweet_text\"].apply(lambda x: replace_contraction(x))\n",
            "<ipython-input-29-15121a3b8744>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"tweet_text_processed\"] = dataset[\"tweet_text_processed\"].apply(lambda x: clean_emoji(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKfjjf0SbNKy"
      },
      "source": [
        "Despues tokenizamos el texto, y trabajaremos en limpiar los tokens que no son útiles en este problema para reducir dimensionalidad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zPH6Gwg6fWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e766ce03-bb72-40e9-d59f-bf45b084cdc1"
      },
      "source": [
        "dataset[\"tokenized\"] = dataset[\"tweet_text_processed\"].apply(lambda x: tokenize(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-e833a41c5a56>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"tokenized\"] = dataset[\"tweet_text_processed\"].apply(lambda x: tokenize(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoiQr_02bcix"
      },
      "source": [
        "Procesamos los tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjGlk7NPbeLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5b775d-0cdc-4601-bf3e-a54efd8bb5c9"
      },
      "source": [
        "# Quitar stopwords\n",
        "dataset[\"tokenized_clean\"] = dataset[\"tokenized\"].apply(lambda x: quitar_stopwords(x))\n",
        "# Quitamos los símbolos de puntuación\n",
        "dataset[\"tokenized_clean\"] = dataset[\"tokenized_clean\"].apply(lambda x: quitar_puntuacion(x))\n",
        "# Lematizamos\n",
        "dataset[\"lematizacion\"] = dataset[\"tokenized_clean\"].apply(lambda x: lematizar(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-803fb01cf205>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"tokenized_clean\"] = dataset[\"tokenized\"].apply(lambda x: quitar_stopwords(x))\n",
            "<ipython-input-31-803fb01cf205>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"tokenized_clean\"] = dataset[\"tokenized_clean\"].apply(lambda x: quitar_puntuacion(x))\n",
            "<ipython-input-31-803fb01cf205>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"lematizacion\"] = dataset[\"tokenized_clean\"].apply(lambda x: lematizar(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "Xx1Rv_HlX4u8",
        "outputId": "d11accae-5983-4500-9373-30f11e85ccd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             tweet_text  molestia  char_len  \\\n",
              "0     Noise harassment is a sensation pain based tor...         1        50   \n",
              "1     It's 4.30am and we still haven't slept because...         1       106   \n",
              "2     These birds acting like I can't grab my chains...         1       143   \n",
              "3     Why do people leave the annoying tap-tap keybo...         1       136   \n",
              "4     Please would you keep the noise down? We're re...         1       109   \n",
              "...                                                 ...       ...       ...   \n",
              "1015  I come alive when I hear your voice it's a bea...         0        81   \n",
              "1016  I'm currently in Ripon, the noise of the thund...         0        61   \n",
              "1017  Sitting down the weir and the noise of the wat...         0        94   \n",
              "1018  The sound of a beer can being cracked open is ...         0        73   \n",
              "1019  I feel so lucky that today is about the quiete...         0       152   \n",
              "\n",
              "     emoji_list  sent_emoji_pos  sent_emoji_neu  sent_emoji_neg  \\\n",
              "0            []        0.000000        0.000000             0.0   \n",
              "1            []        0.000000        0.000000             0.0   \n",
              "2            []        0.000000        0.000000             0.0   \n",
              "3            []        0.000000        0.000000             0.0   \n",
              "4           [🎻]        0.444444        0.555556             0.0   \n",
              "...         ...             ...             ...             ...   \n",
              "1015         []        0.000000        0.000000             0.0   \n",
              "1016         []        0.000000        0.000000             0.0   \n",
              "1017         []        0.000000        0.000000             0.0   \n",
              "1018         []        0.000000        0.000000             0.0   \n",
              "1019         []        0.000000        0.000000             0.0   \n",
              "\n",
              "                                   tweet_text_processed  \\\n",
              "0     Noise harassment is a sensation pain based tor...   \n",
              "1     It is 4.30am and we still have not slept becau...   \n",
              "2     These birds acting like I cannot grab my chain...   \n",
              "3     Why do people leave the annoying tap-tap keybo...   \n",
              "4     Please would you keep the noise down? We are r...   \n",
              "...                                                 ...   \n",
              "1015  I come alive when I hear your voice it is a be...   \n",
              "1016  I am currently in Ripon, the noise of the thun...   \n",
              "1017  Sitting down the weir and the noise of the wat...   \n",
              "1018  The sound of a beer can being cracked open is ...   \n",
              "1019  I feel so lucky that today is about the quiete...   \n",
              "\n",
              "                                              tokenized  \\\n",
              "0     [Noise, harassment, is, a, sensation, pain, ba...   \n",
              "1     [It, is, 4.30, am, and, we, still, have, not, ...   \n",
              "2     [These, birds, acting, like, I, cannot, grab, ...   \n",
              "3     [Why, do, people, leave, the, annoying, tap-ta...   \n",
              "4     [Please, would, you, keep, the, noise, down, ?...   \n",
              "...                                                 ...   \n",
              "1015  [I, come, alive, when, I, hear, your, voice, i...   \n",
              "1016  [I, am, currently, in, Ripon, ,, the, noise, o...   \n",
              "1017  [Sitting, down, the, weir, and, the, noise, of...   \n",
              "1018  [The, sound, of, a, beer, can, being, cracked,...   \n",
              "1019  [I, feel, so, lucky, that, today, is, about, t...   \n",
              "\n",
              "                                        tokenized_clean  \\\n",
              "0     [Noise, harassment, sensation, pain, based, to...   \n",
              "1     [It, still, slept, noise, I, think, I, ever, c...   \n",
              "2     [These, birds, acting, like, I, cannot, grab, ...   \n",
              "3     [Why, people, leave, annoying, keyboard, noise...   \n",
              "4     [Please, would, keep, noise, We, rehearsing, D...   \n",
              "...                                                 ...   \n",
              "1015  [I, come, alive, I, hear, voice, beautiful, so...   \n",
              "1016   [I, currently, Ripon, noise, thunder, fantastic]   \n",
              "1017  [Sitting, weir, noise, water, almost, blocks, ...   \n",
              "1018  [The, sound, beer, cracked, open, greatest, no...   \n",
              "1019  [I, feel, lucky, today, quietest, ever, Fewer,...   \n",
              "\n",
              "                                           lematizacion  \n",
              "0          noise harassment sensation pain base torture  \n",
              "1     it still sleep noise I think I ever city perso...  \n",
              "2     these bird act like I can not grab chainsaw fa...  \n",
              "3     why people leave annoying keyboard noise phone...  \n",
              "4        please would keep noise we rehearse Dartington  \n",
              "...                                                 ...  \n",
              "1015  I come alive I hear voice beautiful sound beau...  \n",
              "1016          I currently ripon noise thunder fantastic  \n",
              "1017     sit weir noise water almost block thought head  \n",
              "1018        the sound beer crack open great noise earth  \n",
              "1019  I feel lucky today quiet ever few copter const...  \n",
              "\n",
              "[1019 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f86c5cf-5237-44e4-9c59-3fcab0a4cb8b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>molestia</th>\n",
              "      <th>char_len</th>\n",
              "      <th>emoji_list</th>\n",
              "      <th>sent_emoji_pos</th>\n",
              "      <th>sent_emoji_neu</th>\n",
              "      <th>sent_emoji_neg</th>\n",
              "      <th>tweet_text_processed</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>tokenized_clean</th>\n",
              "      <th>lematizacion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Noise harassment is a sensation pain based tor...</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Noise harassment is a sensation pain based tor...</td>\n",
              "      <td>[Noise, harassment, is, a, sensation, pain, ba...</td>\n",
              "      <td>[Noise, harassment, sensation, pain, based, to...</td>\n",
              "      <td>noise harassment sensation pain base torture</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It's 4.30am and we still haven't slept because...</td>\n",
              "      <td>1</td>\n",
              "      <td>106</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>It is 4.30am and we still have not slept becau...</td>\n",
              "      <td>[It, is, 4.30, am, and, we, still, have, not, ...</td>\n",
              "      <td>[It, still, slept, noise, I, think, I, ever, c...</td>\n",
              "      <td>it still sleep noise I think I ever city perso...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>These birds acting like I can't grab my chains...</td>\n",
              "      <td>1</td>\n",
              "      <td>143</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>These birds acting like I cannot grab my chain...</td>\n",
              "      <td>[These, birds, acting, like, I, cannot, grab, ...</td>\n",
              "      <td>[These, birds, acting, like, I, cannot, grab, ...</td>\n",
              "      <td>these bird act like I can not grab chainsaw fa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why do people leave the annoying tap-tap keybo...</td>\n",
              "      <td>1</td>\n",
              "      <td>136</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Why do people leave the annoying tap-tap keybo...</td>\n",
              "      <td>[Why, do, people, leave, the, annoying, tap-ta...</td>\n",
              "      <td>[Why, people, leave, annoying, keyboard, noise...</td>\n",
              "      <td>why people leave annoying keyboard noise phone...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Please would you keep the noise down? We're re...</td>\n",
              "      <td>1</td>\n",
              "      <td>109</td>\n",
              "      <td>[🎻]</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Please would you keep the noise down? We are r...</td>\n",
              "      <td>[Please, would, you, keep, the, noise, down, ?...</td>\n",
              "      <td>[Please, would, keep, noise, We, rehearsing, D...</td>\n",
              "      <td>please would keep noise we rehearse Dartington</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>I come alive when I hear your voice it's a bea...</td>\n",
              "      <td>0</td>\n",
              "      <td>81</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>I come alive when I hear your voice it is a be...</td>\n",
              "      <td>[I, come, alive, when, I, hear, your, voice, i...</td>\n",
              "      <td>[I, come, alive, I, hear, voice, beautiful, so...</td>\n",
              "      <td>I come alive I hear voice beautiful sound beau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>I'm currently in Ripon, the noise of the thund...</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>I am currently in Ripon, the noise of the thun...</td>\n",
              "      <td>[I, am, currently, in, Ripon, ,, the, noise, o...</td>\n",
              "      <td>[I, currently, Ripon, noise, thunder, fantastic]</td>\n",
              "      <td>I currently ripon noise thunder fantastic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>Sitting down the weir and the noise of the wat...</td>\n",
              "      <td>0</td>\n",
              "      <td>94</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Sitting down the weir and the noise of the wat...</td>\n",
              "      <td>[Sitting, down, the, weir, and, the, noise, of...</td>\n",
              "      <td>[Sitting, weir, noise, water, almost, blocks, ...</td>\n",
              "      <td>sit weir noise water almost block thought head</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>The sound of a beer can being cracked open is ...</td>\n",
              "      <td>0</td>\n",
              "      <td>73</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>The sound of a beer can being cracked open is ...</td>\n",
              "      <td>[The, sound, of, a, beer, can, being, cracked,...</td>\n",
              "      <td>[The, sound, beer, cracked, open, greatest, no...</td>\n",
              "      <td>the sound beer crack open great noise earth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>I feel so lucky that today is about the quiete...</td>\n",
              "      <td>0</td>\n",
              "      <td>152</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>I feel so lucky that today is about the quiete...</td>\n",
              "      <td>[I, feel, so, lucky, that, today, is, about, t...</td>\n",
              "      <td>[I, feel, lucky, today, quietest, ever, Fewer,...</td>\n",
              "      <td>I feel lucky today quiet ever few copter const...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1019 rows × 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f86c5cf-5237-44e4-9c59-3fcab0a4cb8b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2f86c5cf-5237-44e4-9c59-3fcab0a4cb8b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2f86c5cf-5237-44e4-9c59-3fcab0a4cb8b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLlebJJSfQFR"
      },
      "source": [
        "En este paso también podríamos generar nuevas características para mejorar el funcionamiento del algoritmo. Por ejemplo, podríamos utilizar TextBlob para obtener el sentimiento (tanto subjetividad y polaridad) de cada twitter. Esto se hace de la siguiente forma:\n",
        "\n",
        "\n",
        "```\n",
        "from textblob import TextBlob\n",
        "Textblob(tweet_text).sentiment.subjectivity\n",
        "Textblob(tweet_text).sentiment.polarity\n",
        "```\n",
        "\n",
        "PAra aplicarlo a cada texto de un dataframe habría que utilizar funciones Lambda."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p7hWR7Eu-MM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g_TfWEycll6"
      },
      "source": [
        "## Vectorización\n",
        "\n",
        "Una vez hemos limpiado y procesado el texto, vamos a extraer características utilizando TFIDFVectorizer:\n",
        "- unigramas, bigramas y trigramas\n",
        "- Que el sistema no considere los elementos que salgan en menos del 5% de los documentos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm7g3CEKfuzm"
      },
      "source": [
        "# BoW Features\n",
        "vectorizador = TfidfVectorizer(min_df=0.01, ngram_range = (1,3))\n",
        "vector_data = vectorizador.fit_transform(dataset[\"lematizacion\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-g8b4ZWyAwA",
        "outputId": "a85544e4-c195-4368-840a-366256e02c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1019x181 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 5327 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOyoWLDrdbM0"
      },
      "source": [
        "# Entrenar/testear el clasificador\n",
        "\n",
        "En esta ocasión, además de utilizar las características de *Bag of Ngramas* generadas con TfidfVectorizer, nos interesa utilizar otro conjunto de características que podrían ser de interés para mejorar el rendimiento del clasificador.\n",
        "\n",
        "En este caso, vamos a introducir como ejemplo las variables de sentimiento de emojis que hemos calculado.\n",
        "\n",
        "La forma más sencilla de hacer esto es utilizar la librería *scipy* y generar una matriz sparse, comprensible por scikit-learn, que contenga tanto las características de TFIDF como las calculadas manualmente. \n",
        "\n",
        "En primer lugar, debemos seleccionar el conjunto de variables que queremos considerar en el entrenamiento. PAra ello hacemos uso del selector `dataframe[[\"nombre_columna1\", \"nombre_columna2\"]]`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5przO751gKhT"
      },
      "source": [
        "extra_features = dataset[['sent_emoji_pos','sent_emoji_neg','sent_emoji_neu']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_data.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfn-rmX2OLsQ",
        "outputId": "9500819c-6872-4abc-b0af-72ccd84be78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrIyNSnkekfC"
      },
      "source": [
        "Utilizamos la librería scipy (función sparse.hstack) para unir las características TFIDF (contenidas en ´vector_data´) con las que acabamos de seleccionar (´extra_features´). Esta unión nos generará una matriz X que utilizaremos para hacer el train-test split posteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5-G9GaxCg3e"
      },
      "source": [
        "import scipy as sp\n",
        "# Extraemos las etiquetas y las asignamos a la variable y\n",
        "y = dataset[\"molestia\"].values.astype(np.float32) \n",
        "# Unimos las características TFIDF con las características previamente seleccionadas\n",
        "# Extraemos los valores (values) de las extra_features, que es un dataframe  \n",
        "X = sp.sparse.hstack((vector_data,extra_features.values),format='csr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VojEshiwfjG2"
      },
      "source": [
        "También vamos a extraer el nombre de las caracteríticas por si quisieramos utilizarlos con posterioridad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2e09VPeft_g"
      },
      "source": [
        "X_columns=list(vectorizador.get_feature_names_out())+extra_features.columns.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgaXlfF9f9Q5"
      },
      "source": [
        "Vamos a dividir nuestros datos en Train y Test, como habitualmente se hace. En este caso probablemente tuvieramos demasiadas características (303 características para 764 datos nos va a dar problemas de overfitting, así que en los ejercicios deberías tener esto en cuenta [bajas el número de características])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlbpqmfXfuj9",
        "outputId": "2c0cdf32-f825-407b-ce68-095682bebf13"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(764, 184)\n",
            "(255, 184)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692437DAeiyy"
      },
      "source": [
        "**Decision de modelo de ML a utilizar**\n",
        "\n",
        "En primer lugar se ha generado una función para medir la calidad de varios modelos estándar de forma fácil y ver sus resultados. \n",
        "\n",
        "La función hace un KFold y evalua diferentes modelos con una métrica de evblauación:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos las funcionalidades pertinentes de sklearn:\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "import warnings \n",
        "# Definimos la función encargada de evaluar los modelos:\n",
        "def model_evaluation(models, score, X, y):\n",
        "      results = []\n",
        "      names = []\n",
        "      #PAra cada modelo\n",
        "      for name, model in models:\n",
        "          warnings.filterwarnings('ignore') \n",
        "          # Generamos un Kfold\n",
        "          KF = KFold(n_splits = 10, shuffle = True, random_state = 98)\n",
        "\n",
        "          # hacemos croos_val\n",
        "          cv_results = cross_val_score(model, X, y, cv = KF, scoring = score, verbose = False)\n",
        "          \n",
        "          # Guardamos los resultados:\n",
        "          results.append(cv_results)\n",
        "          names.append(name)\n",
        "          \n",
        "          # Mostramos los resultados numéricamente:\n",
        "          print('Metric: {} , KFold '.format(str(score)))\n",
        "          print(\"%s: %f (%f) \" % (name, cv_results.mean(), cv_results.std()))\n",
        "\n",
        "      return results, names"
      ],
      "metadata": {
        "id": "5m_42mDgO5s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez definida la función, podemos definir los modelos con los que hacer la evaluación. En este caso hemos incorporado la regresión logística y una naive bayes. "
      ],
      "metadata": {
        "id": "R_KYX8MRSzhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los modelos\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Definimos los modelos y generamos una lista con cada uno de ellos:\n",
        "models = [\n",
        "         (\"Logistic\",LogisticRegression(random_state=30)),\n",
        "         (\"GaussianNB\",GaussianNB())\n",
        "]\n",
        "\n",
        "evaluation_score = \"accuracy\"\n",
        "\n",
        "model_evaluation(models,  evaluation_score, X.toarray(), y)   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujjjeM69OU05",
        "outputId": "382b5d30-c1ba-428f-90d1-2b22436fd9b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metric: accuracy , KFold \n",
            "Logistic: 0.785042 (0.047331) \n",
            "Metric: accuracy , KFold \n",
            "GaussianNB: 0.755620 (0.042968) \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([array([0.74509804, 0.83333333, 0.8627451 , 0.83333333, 0.71568627,\n",
              "         0.82352941, 0.78431373, 0.76470588, 0.74509804, 0.74257426]),\n",
              "  array([0.78431373, 0.7254902 , 0.80392157, 0.79411765, 0.64705882,\n",
              "         0.7745098 , 0.76470588, 0.76470588, 0.76470588, 0.73267327])],\n",
              " ['Logistic', 'GaussianNB'])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJu0sOIggTbi"
      },
      "source": [
        "Definimos las variables para hacer una grid_searc:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1OKzEElg44Z"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# define models and parameters\n",
        "model = LogisticRegression()\n",
        "solvers = ['newton-cg', 'liblinear']\n",
        "penalty = ['l2']\n",
        "c_values = [100, 10, 1.0, 0.1, 0.01]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbK9YtNig7Fz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "4c90f118-eb5e-4138-9f7d-0a652c9a1198"
      },
      "source": [
        "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
        "cv = KFold(n_splits=10)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c05ca60f4205>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolvers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1_micro'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'solvers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QONMSvrbghg0"
      },
      "source": [
        "Vamos a entrenar el grid_search para obtener el mejor parámetro para nuestro conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL41eC6sg96m",
        "outputId": "52ab0d28-6ef2-496a-9979-057cbc0cb4a1"
      },
      "source": [
        "grid_result = grid_search.fit(X_train, y_train)\n",
        "# summarize results\n",
        "print(\"Mejor accuracy: %f usando los parámetros %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejor accuracy: 0.770899 usando los parámetros {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha9YcRp-grU7"
      },
      "source": [
        "Entrenamos el modelo con los resultados ofrecidos por la grid_search:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "x-9g5DOMhBeE",
        "outputId": "6603084b-8cb2-4662-da73-c3392902e7c1"
      },
      "source": [
        "from sklearn.model_selection import (KFold, cross_val_score,cross_validate)\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model=LogisticRegression(C=10, penalty=\"l2\", solver = \"newton-cg\")\n",
        "model.fit(X_train,y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, solver='newton-cg')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=10, solver=&#x27;newton-cg&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=10, solver=&#x27;newton-cg&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLGMMi1Ng0na"
      },
      "source": [
        "Vamos a ver como funciona el modelo haciendo el predict del test y mostrando la matriz de confusióñn y el classifciation_Report:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h43h7-Upg04e",
        "outputId": "507bffa2-6324-4eff-e419-e69f03e85cb3"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[105  28]\n",
            " [ 34  88]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.79      0.77       133\n",
            "         1.0       0.76      0.72      0.74       122\n",
            "\n",
            "    accuracy                           0.76       255\n",
            "   macro avg       0.76      0.76      0.76       255\n",
            "weighted avg       0.76      0.76      0.76       255\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jja7IS6vg_dp"
      },
      "source": [
        "Además, podríamos mostrar el grado de importancia relativa de las variables dle modelo. Aquí hago el listado, pero lo ideal sería seleccionar las más importantes dentro del modelo para saber cuales están teniendo más influencia:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb0sU6ANFBUa"
      },
      "source": [
        "# Obtener la importancia de las variables del modelo\n",
        "importance = model.coef_[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VAyddZlhbjh"
      },
      "source": [
        "A continuación utilizamos esa variable de importancia de variables, junto a los nombres de las características almacenadas anteriormente en X_columns, para listar la importancia de cada una de las variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxj-SgYChb3_"
      },
      "source": [
        "# Mostrar el número de la característica, con su nombre, y su score de importancia\n",
        "for i,v in enumerate(importance):\n",
        " print('Feature: %0d, Name: %s , Score: %.5f' % (i,X_columns[i],v))\n",
        "# plot feature importance\n",
        "plt.bar([x for x in range(len(importance))], importance)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}