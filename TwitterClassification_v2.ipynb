{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Zi-MaTRZZQ3q",
        "JMGuxTOkarOY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSCQn96RT3mU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e03eaa6-3bda-4fc4-d0cd-93ce37dfc92b"
      },
      "source": [
        "!wget https://github.com/luisgasco/ntic_master_datos/raw/main/datasets/datos_twitter_master.tsv\n",
        "!pip install emoji_extractor\n",
        "!pip install emoji\n",
        "import sys  \n",
        "!{sys.executable} -m pip install contractions\n",
        "!pip install spacy==3.2.1\n",
        "!python -m spacy download en_core_web_sm\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# Download emoji sentiment\n",
        "!wget https://www.clarin.si/repository/xmlui/handle/11356/1048/allzip\n",
        "!unzip allzip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-08 17:09:25--  https://github.com/luisgasco/ntic_master_datos/raw/main/datasets/datos_twitter_master.tsv\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/luisgasco/ntic_master_datos/main/datasets/datos_twitter_master.tsv [following]\n",
            "--2023-05-08 17:09:26--  https://raw.githubusercontent.com/luisgasco/ntic_master_datos/main/datasets/datos_twitter_master.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 113587 (111K) [text/plain]\n",
            "Saving to: ‚Äòdatos_twitter_master.tsv‚Äô\n",
            "\n",
            "datos_twitter_maste 100%[===================>] 110.92K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-05-08 17:09:26 (6.76 MB/s) - ‚Äòdatos_twitter_master.tsv‚Äô saved [113587/113587]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji_extractor\n",
            "  Downloading emoji_extractor-2.0.0.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji_extractor\n",
            "  Building wheel for emoji_extractor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji_extractor: filename=emoji_extractor-2.0.0-py3-none-any.whl size=64178 sha256=86bc4b6d1337738c810d6b913e9e50a656fe91dd18075c08d9e335b581f7f5b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/89/95/33d1b161e1daa8fb2dec089e887d3620d3a318fdb0798e9f58\n",
            "Successfully built emoji_extractor\n",
            "Installing collected packages: emoji_extractor\n",
            "Successfully installed emoji_extractor-2.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=dd89cd31147404be6a81fed725e6afe1b31a18d5ffabf0e602b4a5bc522d96a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/3d/88/51a592b9ad17e7899126563698b4e3961983ebe85747228ba6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==3.2.1\n",
            "  Downloading spacy-3.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (2.0.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (0.7.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (3.0.8)\n",
            "Collecting wasabi<1.1.0,>=0.8.1\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (0.10.1)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (3.3.0)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (659 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m659.5/659.5 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (2.27.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (1.22.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (2.0.7)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (23.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (4.65.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (2.4.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (1.0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy==3.2.1) (3.1.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy==3.2.1) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy==3.2.1) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.1) (2.0.12)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy==3.2.1) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy==3.2.1) (2.1.2)\n",
            "Installing collected packages: wasabi, typer, pydantic, thinc, spacy\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.1\n",
            "    Uninstalling wasabi-1.1.1:\n",
            "      Successfully uninstalled wasabi-1.1.1\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.7.0\n",
            "    Uninstalling typer-0.7.0:\n",
            "      Successfully uninstalled typer-0.7.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.7\n",
            "    Uninstalling pydantic-1.10.7:\n",
            "      Successfully uninstalled pydantic-1.10.7\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.9\n",
            "    Uninstalling thinc-8.1.9:\n",
            "      Successfully uninstalled thinc-8.1.9\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 6.0.4 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n",
            "en-core-web-sm 3.5.0 requires spacy<3.6.0,>=3.5.0, but you have spacy 3.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-1.8.2 spacy-3.2.1 thinc-8.0.17 typer-0.4.2 wasabi-0.10.1\n",
            "2023-05-08 17:10:47.017640: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-08 17:10:51.246832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[33mDEPRECATION: https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl#egg=en_core_web_sm==3.2.0 contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (67.7.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.22.4)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.1.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.65.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.12)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.27.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.8)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (23.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (6.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 3.5.0\n",
            "    Uninstalling en-core-web-sm-3.5.0:\n",
            "      Successfully uninstalled en-core-web-sm-3.5.0\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-08 17:11:04--  https://www.clarin.si/repository/xmlui/handle/11356/1048/allzip\n",
            "Resolving www.clarin.si (www.clarin.si)... 95.87.154.205\n",
            "Connecting to www.clarin.si (www.clarin.si)|95.87.154.205|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‚Äòallzip‚Äô\n",
            "\n",
            "allzip                  [   <=>              ]  94.61K  79.0KB/s    in 1.2s    \n",
            "\n",
            "2023-05-08 17:11:09 (79.0 KB/s) - ‚Äòallzip‚Äô saved [96878]\n",
            "\n",
            "Archive:  allzip\n",
            "  inflating: ESR_v1.0_format.txt     \n",
            "  inflating: Emoji_Sentiment_Data_v1.0.csv  \n",
            "  inflating: Emojitracker_20150604.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL_KPNOkORKV"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOe_gsA_x9Eh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import contractions\n",
        "import re\n",
        "from emoji_extractor.extract import Extractor\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import en_core_web_sm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8WynRVxPU0z"
      },
      "source": [
        "# Funciones que se utilizar√°n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpGYyDY5x73m"
      },
      "source": [
        "# Preparar diccionario de emojis\n",
        "def load_emoji_sentiment(path):\n",
        "  # Cargamos el csv de emoji_sentiment\n",
        "  emoji_sent_df = pd.read_csv(path,sep=\",\")\n",
        "  # Calculamos los scores dividiendo el n√∫mero de emojis negativos y entre el total\n",
        "  emoji_sent_df[\"Negative\"] = emoji_sent_df[\"Negative\"]/emoji_sent_df[\"Occurrences\"]\n",
        "  emoji_sent_df[\"Neutral\"] = emoji_sent_df[\"Neutral\"]/emoji_sent_df[\"Occurrences\"]\n",
        "  emoji_sent_df[\"Positive\"] = emoji_sent_df[\"Positive\"]/emoji_sent_df[\"Occurrences\"]\n",
        "  # Transformamos a dict\n",
        "  emoji_sent_df = emoji_sent_df.set_index('Emoji')\n",
        "  emoji_dict = emoji_sent_df.to_dict(orient=\"index\")\n",
        "  return emoji_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xtfAvxiPmRM"
      },
      "source": [
        "# Reemplazar contracciones y slang en ingl√©s usando la librer√≠a \"contractions\" https://github.com/kootenpv/contractions\n",
        "def replace_contraction(text):\n",
        "    expanded_words = []\n",
        "    # Divide el texto\n",
        "    for t in text.split():\n",
        "        # Aplica la funci√≥n fix en cada secci√≥n o token del texto buscando contracciones y slang\n",
        "        expanded_words.append(contractions.fix(t, slang = True))\n",
        "    expanded_text = ' '.join(expanded_words) \n",
        "    return expanded_text\n",
        "\n",
        "# Hay un tokenizador guay para twitter https://github.com/jaredks/tweetokenize\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7fGZmT1QWxi"
      },
      "source": [
        "# Funci√≥n para extraer emojis del texto en formato lista\n",
        "def extract_emojis(text):\n",
        "  extract = Extractor()\n",
        "  emojis = extract.count_emoji(text, check_first=False)\n",
        "  emojis_list = [key for key, _ in emojis.most_common()]\n",
        "  return emojis_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfDZavZWRrr2"
      },
      "source": [
        "# Calcula el sentimiento de los emojis de una lista utilizando el diccionario\n",
        "# de emoji sentiment score generado previamente con la funci√≥n load_emoji_sentiment()\n",
        "# Se puede extraer el valor de positividad de los emojis con la option \"positive\"\n",
        "# Se puede extraer el valor de neutralidad de los emojis con la option \"neutral\"\"  \n",
        "# Se puede extraer el valor de e negatividad de los emojis con la option \"negative\"\"  \n",
        "\n",
        "def get_emoji_sentiment(lista, option = \"positive\"):\n",
        "  output = 0\n",
        "  for emoji in lista:\n",
        "    try:\n",
        "      if option == \"positive\":\n",
        "        output = output + emoji_sent_dict[emoji][\"Positive\"]\n",
        "      elif option ==\"negative\":\n",
        "        output = output + emoji_sent_dict[emoji][\"Negative\"]\n",
        "      elif option ==\"neutral\":\n",
        "        output = output + emoji_sent_dict[emoji][\"Neutral\"]\n",
        "    except Exception as e: \n",
        "      continue\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iidMorvfQX8g"
      },
      "source": [
        "# Eliminar los emojis de un texto. Esto es √∫til porque una vez extraido los emojis\n",
        "# puede interesarnos tener un texto sin presencia de emojis para mejor an√°lisis.\n",
        "def clean_emoji(text):\n",
        "    # Poner todos los comandos de http://www.unicode.org/Public/emoji/1.0/emoji-data.txt\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F300-\\U0001F6FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u'\\u2600-\\u26FF\\u2700-\\u27BF'\n",
        "        u'\\u2934' u'\\u2935' u'\\u2B05' u'\\u2B06' u'\\u2B07' u'\\u2B1B' u'\\u2B1C' \n",
        "        u'\\u2B50' u'\\u2B55' u'\\u3030' u'\\u303D' u'\\u3297' u'\\u3299' u'\\u00A9'\n",
        "        u'\\u00AE' u'\\u203C' u'\\u2049' u'\\u2122' u'\\u2139' u'\\u2194-\\u2199' \n",
        "        u'\\u21A9' u'\\u21AA' u'\\u231A' u'\\u231B' u'\\u2328' u'\\u23CF'\n",
        "        u'\\u23E9-\\u23F3' u'\\u23F8' u'\\u23F9' u'\\u23FA' u'\\u24C2' u'\\u25AA'\n",
        "        u'\\u25AB' u'\\u25B6' u'\\u25C0' u'\\u25FB' u'\\u25FD' u'\\u25FC' u'\\u25FE'\n",
        "        ']+', flags=re.UNICODE)\n",
        "    string2 = re.sub(emoji_pattern,r' ',text)\n",
        "    return string2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFvpD7AoQ_BI"
      },
      "source": [
        "# Tokenizar los tweets con el tokenizador \"TweetTokenizer\" de NLTK\n",
        "def tokenize(texto):\n",
        "  tweet_tokenizer = TweetTokenizer()\n",
        "  tokens_list = tweet_tokenizer.tokenize(texto)\n",
        "  return tokens_list\n",
        "\n",
        "# Quitar stop words de una lista de tokens\n",
        "def quitar_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    filtered_sentence = [w for w in tokens if not w in stop_words]\n",
        "    return filtered_sentence\n",
        "\n",
        "\n",
        "# Eliminar signos de puntuaci√≥n de una lista de tokens\n",
        "# (nos quedamos s√≥lo lo alfanum√©rico en este caso)\n",
        "def quitar_puntuacion(tokens):\n",
        "    words=[word for word in tokens if word.isalnum()]\n",
        "    return words\n",
        "\n",
        "\n",
        "# Lemmatization de los tokens. Devuelve una string entera para hacer la tokenizaci√≥n\n",
        "# con NLTK\n",
        "nlp = en_core_web_sm.load(disable=['parser', 'ner'])\n",
        "def lematizar(tokens):\n",
        "    sentence = \" \".join(tokens)\n",
        "    mytokens = nlp(sentence)\n",
        "    # Lematizamos los tokens y los convertimos  a minusculas\n",
        "    mytokens = [ word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "    # Extraemos el text en una string\n",
        "    return \" \".join(mytokens)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkJhe7nYTRVG"
      },
      "source": [
        "# Cargar y preparar los datos\n",
        "En primer lugar cargamos los datos que vamos a utilizar en este notebook (emoji_sentiment_data y datos de twitter)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ID1bUphyILE"
      },
      "source": [
        "emoji_sent_dict = load_emoji_sentiment(\"Emoji_Sentiment_Data_v1.0.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOvvCLWJT1Ll"
      },
      "source": [
        "Podemos textear que hemos obtenido un diccionario cuyas claves son los emojis presentes dentro de emoji sentiment score. Cada emoji tiene un score de negatividad, neutralidad, positividad y otros campos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok9ozYguVFMd",
        "outputId": "77ae65d6-ccfc-485b-962a-df21e847a099"
      },
      "source": [
        "emoji_sent_dict[\"üò≠\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Unicode codepoint': '0x1f62d',\n",
              " 'Occurrences': 5526,\n",
              " 'Position': 0.803351976,\n",
              " 'Negative': 0.4364820846905538,\n",
              " 'Neutral': 0.22041259500542887,\n",
              " 'Positive': 0.34310532030401736,\n",
              " 'Unicode name': 'LOUDLY CRYING FACE',\n",
              " 'Unicode block': 'Emoticons'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTQVDcH7VpxO"
      },
      "source": [
        "Cargamos el fichero .tsv con los datos de Twitter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "9Am6BD7ET-NB",
        "outputId": "95b737d5-cfa5-4bf3-d936-1f9fa9621cb1"
      },
      "source": [
        "dataset = pd.read_csv(\"datos_twitter_master.tsv\", sep=\"\\t\")\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             tweet_text  molestia\n",
              "0     Noise harassment is a sensation pain based tor...         1\n",
              "1     It's 4.30am and we still haven't slept because...         1\n",
              "2     These birds acting like I can't grab my chains...         1\n",
              "3     Why do people leave the annoying tap-tap keybo...         1\n",
              "4     Please would you keep the noise down? We're re...         1\n",
              "...                                                 ...       ...\n",
              "1015  I come alive when I hear your voice it's a bea...         0\n",
              "1016  I'm currently in Ripon, the noise of the thund...         0\n",
              "1017  Sitting down the weir and the noise of the wat...         0\n",
              "1018  The sound of a beer can being cracked open is ...         0\n",
              "1019  I feel so lucky that today is about the quiete...         0\n",
              "\n",
              "[1020 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c29160d3-4ff9-4205-9367-775880b35244\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>molestia</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Noise harassment is a sensation pain based tor...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It's 4.30am and we still haven't slept because...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>These birds acting like I can't grab my chains...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why do people leave the annoying tap-tap keybo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Please would you keep the noise down? We're re...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>I come alive when I hear your voice it's a bea...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>I'm currently in Ripon, the noise of the thund...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>Sitting down the weir and the noise of the wat...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>The sound of a beer can being cracked open is ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>I feel so lucky that today is about the quiete...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1020 rows √ó 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c29160d3-4ff9-4205-9367-775880b35244')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c29160d3-4ff9-4205-9367-775880b35244 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c29160d3-4ff9-4205-9367-775880b35244');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXS6rO_8X_gF"
      },
      "source": [
        "# An√°lisis preliminar "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcxAkTt6RG2U"
      },
      "source": [
        "# An√°lisis exploratorio de los datos (EDA)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5ZZJJilSUbe"
      },
      "source": [
        "En este apartado pretendemos realizar un an√°lisis de los datos previo a la normalizaci√≥n de los mismos. Este an√°lisis nos va a permitir extraer informaci√≥n relevante del dataset, as√≠ como posibles inconvenientes que ser√°n solucionados llegado el caso.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGFFWS9Q5uPg"
      },
      "source": [
        "* **N√∫mero de documentos y columnas:**\n",
        "\n",
        "Comenzamos mostrando el n√∫mero de documentos, o lo que es lo mismo, el n√∫mero de filas del data frame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cMr_JLnXyLv",
        "outputId": "db6e7ff2-3d70-4135-923a-d6aec174c1bb"
      },
      "source": [
        "print(\"Tenemos un conjunto de {} documentos\".format(len(dataset)))\n",
        "print(\"El dataframe tiene {} columnas\".format(dataset.shape[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tenemos un conjunto de 1020 documentos\n",
            "El dataframe tiene 2 columnas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **N√∫mero de documentos duplicados:**"
      ],
      "metadata": {
        "id": "TOwki0MDKR9c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUMrCNQLWnER"
      },
      "source": [
        "Despues, comprobamos y eliminamos las filas con alg√∫n valor vac√≠o (NA) y quitaremos los duplicados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlTJyjagXNIK",
        "outputId": "c3dd5b3c-edac-4e4e-a948-3a5d46efa4a3"
      },
      "source": [
        "print(\"Existen {} noticias duplicadas\".format(np.sum(dataset.duplicated(subset=[\"tweet_text\"]))))\n",
        "# Quitaremos esos duplicados\n",
        "dataset = dataset.drop_duplicates()\n",
        "print(\"Despues de quitar duplicados tenemos un conjunto de {} noticias\".format(dataset.shape[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existen 1 noticias duplicadas\n",
            "Despues de quitar duplicados tenemos un conjunto de 1019 noticias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR2fqxu6WrGo"
      },
      "source": [
        "Comprobaramos que no hayan quedado Nulls en ningunas de las dos columnas del dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEvobTAsXmMy",
        "outputId": "36269153-a18f-4f0c-97d7-22d1ef535077"
      },
      "source": [
        "print(\"Hay {} valores vac√≠os en las noticias y {} valores vac√≠os en las etiquetas en los datos\".format(np.sum(dataset.isnull())[0],\n",
        "                                                                                                        np.sum(dataset.isnull())[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hay 0 valores vac√≠os en las noticias y 0 valores vac√≠os en las etiquetas en los datos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **N√∫mero de documentos por cada clase:**\n",
        "\n",
        "Contamos el n√∫mero de elementos de cada clase. Vemos que en la columna \"molestia\" nos encontramos las etiquetas del dataset. En este caso nos encontramos dos tipos de documentos (tweets):\n",
        "\n",
        "- \"Molestia = 1\": Tweets con la palabra ruido que hacen referencia a molestias sufridas por ruido ac√∫stico proveniente de distintas fuentes (coches, vecinos, mascotas,...)\n",
        "- \"Molestia = 0\": Tweets que contienen la palabra ruido perso no expresan una molestia sufrida por el usuario que lo escribi√≥ (otras acpciones de ruido, noticias que hablan sobre ruido o uso de ruido como algo positivo) "
      ],
      "metadata": {
        "id": "h24AMcCZKnbw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY8ksiYvZvqt"
      },
      "source": [
        "Comprobemos la distribuci√≥n de las clases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ead7ae00-41f9-438e-c06a-d300cca7f57f",
        "id": "U1AYyDfTZvqu"
      },
      "source": [
        "dataset[\"molestia\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    510\n",
              "0    509\n",
              "Name: molestia, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvLsb-_gaFyH"
      },
      "source": [
        "¬°¬°Tenemos un dataset balanceado!! Esto nos evitar√° problemas en el entrenamiento de los modelosüòÄ. \n",
        "\n",
        "Disponemos 509 noticias verdaderas (valor 0) y 29571 noticias falsas (valor 1).\n",
        "\n",
        "Vamos a dibujar un histograma con las clases as√≠ practicamos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "imbF43WHaFOz",
        "outputId": "db544d44-fd1a-487c-ec58-10adcf3e8b38"
      },
      "source": [
        "ax, fig = plt.subplots()\n",
        "etiquetas = dataset.molestia.value_counts()\n",
        "etiquetas.plot(kind= 'bar', color= [\"blue\", \"orange\"])\n",
        "plt.title('Bar chart')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ors90Yon5aJf"
      },
      "source": [
        "* **Distribuci√≥n de la longitud de los tweet en caracteres:**\n",
        "\n",
        "Para seguir con el an√°lisis exploratorio, vamos a hacer un c√°lculo t√≠pico: la longitud de cada uno de los textos de los documentos para despues dibujar su histograma. \n",
        "\n",
        "Comenzamos creando las columnas que van a almacenar las longitud en caracteres y en tokens de los documentos del corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVjO14rKXWXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e97818-3395-43d2-cd73-508cfe334ef1"
      },
      "source": [
        "dataset[\"char_len\"] = dataset[\"tweet_text\"].apply(lambda x: len(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-6bd16aaca85c>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"char_len\"] = dataset[\"tweet_text\"].apply(lambda x: len(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librer√≠as matplotlib y seaborn:\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig = plt.figure(figsize=(14,12))\n",
        "sns.set_style(\"darkgrid\")\n",
        "# a√±adimos series para cada categor√≠a (eligiendo la ser√≠e de char_len\n",
        "plt1 = sns.distplot(dataset[dataset[\"molestia\"]==0].char_len, hist=True, label=\"no_molestia\")\n",
        "plt2 = sns.distplot(dataset[dataset[\"molestia\"]==1].char_len, hist=True, label=\"molestia\")\n",
        "fig.legend(labels=['no molestia','molestia'], loc = 5)\n",
        "\n",
        "\n",
        "# Definimos el t√≠tulo de los ejes:\n",
        "plt.xlabel('Caracteres', fontsize=16)\n",
        "plt.ylabel('Densidad', fontsize=16)\n",
        "\n",
        "# Finalmente mostramos el gr√°fico:\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9Qh8tMrcLSmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKMCoo9-XnmX"
      },
      "source": [
        "En la figura se ve que no existen diferencias significativas entre las dos clases. Quiz√° los tweets en los que el usuario se queja sobre el ruido (molestia ==1)tienen una tendencia a ser m√°s cortos, pero no se observa nada destacable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YURv-wywYDpq"
      },
      "source": [
        "# Transformaci√≥n \n",
        "\n",
        "Como hemos visto, est√° dividido en dos pasos Normalizaci√≥n o Preprocesado y Transformaci√≥n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG9xxARQYJ_8"
      },
      "source": [
        "\n",
        "## Normalizaci√≥n\n",
        "Vamos a proceder a normalizar los datos. Para ello vamos a utilizar las funciones anteriormente definidas:\n",
        "\n",
        "- Por una parte vamos a extraer los emojis de los tweets, los vamos a guardar en una lista dentro de una nueva columna del dataframe y por √∫ltimo calcularemos un valor de sentimiento de emojis de positividad, negatividad y neutralidad.\n",
        "\n",
        "- Preprocesar los textos:\n",
        "    - Primero expanderemos las contracciones de los tweets\n",
        "    - Despues quitaremos los emojis, ya que antes habremos calculado los scores necesarios.\n",
        "    - Tokenizaremos\n",
        "    - Quitaremos stop words\n",
        "    - Quitaremos puntuaci√≥n\n",
        "    - Lematizaremos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lematizar(tokens):\n",
        "    sentence = \" \".join(tokens)\n",
        "    mytokens = nlp(sentence)\n",
        "    # Lematizamos los tokens y los convertimos  a minusculas\n",
        "    mytokens = [ word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "    # Extraemos el text en una string\n",
        "    return \" \".join(mytokens)"
      ],
      "metadata": {
        "id": "epXWuKOdYoBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi-MaTRZZQ3q"
      },
      "source": [
        "### Emojis\n",
        "En primer luigar vamos a trabajar con los emojis. \n",
        "\n",
        "Vamos a extraerlos con una funci√≥n lambda aplicando la funci√≥n extract_emojis() definida anteriormente en el dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ7t-9ZQYX6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27887937-7ce7-4441-b7da-6ccb66b2d837"
      },
      "source": [
        "dataset[\"emoji_list\"] = dataset[\"tweet_text\"].apply(lambda x: extract_emojis(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-bb213c0cc61f>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"emoji_list\"] = dataset[\"tweet_text\"].apply(lambda x: extract_emojis(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od_3GHZsZwaz"
      },
      "source": [
        "Vemos que nos ha guardado los emojis en la columna \"emoji_list\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxW7SeDIZwAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3445e500-afc0-46cc-e88f-80266e487d89"
      },
      "source": [
        "dataset[\"emoji_list\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        []\n",
              "1        []\n",
              "2        []\n",
              "3        []\n",
              "4       [üéª]\n",
              "       ... \n",
              "1015     []\n",
              "1016     []\n",
              "1017     []\n",
              "1018     []\n",
              "1019     []\n",
              "Name: emoji_list, Length: 1019, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akaI1mmAaHbl"
      },
      "source": [
        "A continuaci√≥n, se calcula un score de sentimiento a los emojis asociados a cada tweet. Si no hay emojis, estos scores ser√°n cero.\n",
        "Para calcular esto lo haremos de nuevo con funciones lambda aplicando la funci√≥n get_emoji_sentiment() anteriormente generada:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzdd6JrlaYeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d450701e-b683-4c55-aea4-c051c3385aab"
      },
      "source": [
        "dataset[\"sent_emoji_pos\"] = dataset[\"emoji_list\"].apply(lambda x: get_emoji_sentiment(x, \"positive\"))\n",
        "dataset[\"sent_emoji_neu\"] = dataset[\"emoji_list\"].apply(lambda x: get_emoji_sentiment(x, \"neutral\"))\n",
        "dataset[\"sent_emoji_neg\"] = dataset[\"emoji_list\"].apply(lambda x: get_emoji_sentiment(x, \"negative\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-bae707eba02b>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"sent_emoji_pos\"] = dataset[\"emoji_list\"].apply(lambda x: get_emoji_sentiment(x, \"positive\"))\n",
            "<ipython-input-27-bae707eba02b>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"sent_emoji_neu\"] = dataset[\"emoji_list\"].apply(lambda x: get_emoji_sentiment(x, \"neutral\"))\n",
            "<ipython-input-27-bae707eba02b>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"sent_emoji_neg\"] = dataset[\"emoji_list\"].apply(lambda x: get_emoji_sentiment(x, \"negative\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI2Ekz7SZulT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897e6ee0-4bbd-442b-b988-cb9538a35a7f"
      },
      "source": [
        "dataset[\"sent_emoji_pos\"].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    1019.000000\n",
              "mean        0.063655\n",
              "std         0.194156\n",
              "min         0.000000\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         0.000000\n",
              "max         2.175897\n",
              "Name: sent_emoji_pos, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMGuxTOkarOY"
      },
      "source": [
        "### Preprocesar textos\n",
        "Vamos a realizar los preprocesados indicados antes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPfOeLYua43J"
      },
      "source": [
        " En primer lugar expandimos las contracciones. Adem√°s, despues del proceso de extracci√≥n de emojis, los quitaremos de nuestros textos porque no nos ser√°n √∫tiles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dru7LpIBZ31S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78134c15-9902-4c1c-8c02-c4a54186b60a"
      },
      "source": [
        "# Reemplazar contracciones\n",
        "dataset[\"tweet_text_processed\"] = dataset[\"tweet_text\"].apply(lambda x: replace_contraction(x))\n",
        "# Quitar emojis de los textos\n",
        "dataset[\"tweet_text_processed\"] = dataset[\"tweet_text_processed\"].apply(lambda x: clean_emoji(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-15121a3b8744>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"tweet_text_processed\"] = dataset[\"tweet_text\"].apply(lambda x: replace_contraction(x))\n",
            "<ipython-input-29-15121a3b8744>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"tweet_text_processed\"] = dataset[\"tweet_text_processed\"].apply(lambda x: clean_emoji(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKfjjf0SbNKy"
      },
      "source": [
        "Despues tokenizamos el texto, y trabajaremos en limpiar los tokens que no son √∫tiles en este problema para reducir dimensionalidad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zPH6Gwg6fWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e766ce03-bb72-40e9-d59f-bf45b084cdc1"
      },
      "source": [
        "dataset[\"tokenized\"] = dataset[\"tweet_text_processed\"].apply(lambda x: tokenize(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-e833a41c5a56>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"tokenized\"] = dataset[\"tweet_text_processed\"].apply(lambda x: tokenize(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoiQr_02bcix"
      },
      "source": [
        "Procesamos los tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjGlk7NPbeLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c5b775d-0cdc-4601-bf3e-a54efd8bb5c9"
      },
      "source": [
        "# Quitar stopwords\n",
        "dataset[\"tokenized_clean\"] = dataset[\"tokenized\"].apply(lambda x: quitar_stopwords(x))\n",
        "# Quitamos los s√≠mbolos de puntuaci√≥n\n",
        "dataset[\"tokenized_clean\"] = dataset[\"tokenized_clean\"].apply(lambda x: quitar_puntuacion(x))\n",
        "# Lematizamos\n",
        "dataset[\"lematizacion\"] = dataset[\"tokenized_clean\"].apply(lambda x: lematizar(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-803fb01cf205>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"tokenized_clean\"] = dataset[\"tokenized\"].apply(lambda x: quitar_stopwords(x))\n",
            "<ipython-input-31-803fb01cf205>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"tokenized_clean\"] = dataset[\"tokenized_clean\"].apply(lambda x: quitar_puntuacion(x))\n",
            "<ipython-input-31-803fb01cf205>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"lematizacion\"] = dataset[\"tokenized_clean\"].apply(lambda x: lematizar(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "Xx1Rv_HlX4u8",
        "outputId": "d11accae-5983-4500-9373-30f11e85ccd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             tweet_text  molestia  char_len  \\\n",
              "0     Noise harassment is a sensation pain based tor...         1        50   \n",
              "1     It's 4.30am and we still haven't slept because...         1       106   \n",
              "2     These birds acting like I can't grab my chains...         1       143   \n",
              "3     Why do people leave the annoying tap-tap keybo...         1       136   \n",
              "4     Please would you keep the noise down? We're re...         1       109   \n",
              "...                                                 ...       ...       ...   \n",
              "1015  I come alive when I hear your voice it's a bea...         0        81   \n",
              "1016  I'm currently in Ripon, the noise of the thund...         0        61   \n",
              "1017  Sitting down the weir and the noise of the wat...         0        94   \n",
              "1018  The sound of a beer can being cracked open is ...         0        73   \n",
              "1019  I feel so lucky that today is about the quiete...         0       152   \n",
              "\n",
              "     emoji_list  sent_emoji_pos  sent_emoji_neu  sent_emoji_neg  \\\n",
              "0            []        0.000000        0.000000             0.0   \n",
              "1            []        0.000000        0.000000             0.0   \n",
              "2            []        0.000000        0.000000             0.0   \n",
              "3            []        0.000000        0.000000             0.0   \n",
              "4           [üéª]        0.444444        0.555556             0.0   \n",
              "...         ...             ...             ...             ...   \n",
              "1015         []        0.000000        0.000000             0.0   \n",
              "1016         []        0.000000        0.000000             0.0   \n",
              "1017         []        0.000000        0.000000             0.0   \n",
              "1018         []        0.000000        0.000000             0.0   \n",
              "1019         []        0.000000        0.000000             0.0   \n",
              "\n",
              "                                   tweet_text_processed  \\\n",
              "0     Noise harassment is a sensation pain based tor...   \n",
              "1     It is 4.30am and we still have not slept becau...   \n",
              "2     These birds acting like I cannot grab my chain...   \n",
              "3     Why do people leave the annoying tap-tap keybo...   \n",
              "4     Please would you keep the noise down? We are r...   \n",
              "...                                                 ...   \n",
              "1015  I come alive when I hear your voice it is a be...   \n",
              "1016  I am currently in Ripon, the noise of the thun...   \n",
              "1017  Sitting down the weir and the noise of the wat...   \n",
              "1018  The sound of a beer can being cracked open is ...   \n",
              "1019  I feel so lucky that today is about the quiete...   \n",
              "\n",
              "                                              tokenized  \\\n",
              "0     [Noise, harassment, is, a, sensation, pain, ba...   \n",
              "1     [It, is, 4.30, am, and, we, still, have, not, ...   \n",
              "2     [These, birds, acting, like, I, cannot, grab, ...   \n",
              "3     [Why, do, people, leave, the, annoying, tap-ta...   \n",
              "4     [Please, would, you, keep, the, noise, down, ?...   \n",
              "...                                                 ...   \n",
              "1015  [I, come, alive, when, I, hear, your, voice, i...   \n",
              "1016  [I, am, currently, in, Ripon, ,, the, noise, o...   \n",
              "1017  [Sitting, down, the, weir, and, the, noise, of...   \n",
              "1018  [The, sound, of, a, beer, can, being, cracked,...   \n",
              "1019  [I, feel, so, lucky, that, today, is, about, t...   \n",
              "\n",
              "                                        tokenized_clean  \\\n",
              "0     [Noise, harassment, sensation, pain, based, to...   \n",
              "1     [It, still, slept, noise, I, think, I, ever, c...   \n",
              "2     [These, birds, acting, like, I, cannot, grab, ...   \n",
              "3     [Why, people, leave, annoying, keyboard, noise...   \n",
              "4     [Please, would, keep, noise, We, rehearsing, D...   \n",
              "...                                                 ...   \n",
              "1015  [I, come, alive, I, hear, voice, beautiful, so...   \n",
              "1016   [I, currently, Ripon, noise, thunder, fantastic]   \n",
              "1017  [Sitting, weir, noise, water, almost, blocks, ...   \n",
              "1018  [The, sound, beer, cracked, open, greatest, no...   \n",
              "1019  [I, feel, lucky, today, quietest, ever, Fewer,...   \n",
              "\n",
              "                                           lematizacion  \n",
              "0          noise harassment sensation pain base torture  \n",
              "1     it still sleep noise I think I ever city perso...  \n",
              "2     these bird act like I can not grab chainsaw fa...  \n",
              "3     why people leave annoying keyboard noise phone...  \n",
              "4        please would keep noise we rehearse Dartington  \n",
              "...                                                 ...  \n",
              "1015  I come alive I hear voice beautiful sound beau...  \n",
              "1016          I currently ripon noise thunder fantastic  \n",
              "1017     sit weir noise water almost block thought head  \n",
              "1018        the sound beer crack open great noise earth  \n",
              "1019  I feel lucky today quiet ever few copter const...  \n",
              "\n",
              "[1019 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f86c5cf-5237-44e4-9c59-3fcab0a4cb8b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>molestia</th>\n",
              "      <th>char_len</th>\n",
              "      <th>emoji_list</th>\n",
              "      <th>sent_emoji_pos</th>\n",
              "      <th>sent_emoji_neu</th>\n",
              "      <th>sent_emoji_neg</th>\n",
              "      <th>tweet_text_processed</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>tokenized_clean</th>\n",
              "      <th>lematizacion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Noise harassment is a sensation pain based tor...</td>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Noise harassment is a sensation pain based tor...</td>\n",
              "      <td>[Noise, harassment, is, a, sensation, pain, ba...</td>\n",
              "      <td>[Noise, harassment, sensation, pain, based, to...</td>\n",
              "      <td>noise harassment sensation pain base torture</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It's 4.30am and we still haven't slept because...</td>\n",
              "      <td>1</td>\n",
              "      <td>106</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>It is 4.30am and we still have not slept becau...</td>\n",
              "      <td>[It, is, 4.30, am, and, we, still, have, not, ...</td>\n",
              "      <td>[It, still, slept, noise, I, think, I, ever, c...</td>\n",
              "      <td>it still sleep noise I think I ever city perso...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>These birds acting like I can't grab my chains...</td>\n",
              "      <td>1</td>\n",
              "      <td>143</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>These birds acting like I cannot grab my chain...</td>\n",
              "      <td>[These, birds, acting, like, I, cannot, grab, ...</td>\n",
              "      <td>[These, birds, acting, like, I, cannot, grab, ...</td>\n",
              "      <td>these bird act like I can not grab chainsaw fa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why do people leave the annoying tap-tap keybo...</td>\n",
              "      <td>1</td>\n",
              "      <td>136</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Why do people leave the annoying tap-tap keybo...</td>\n",
              "      <td>[Why, do, people, leave, the, annoying, tap-ta...</td>\n",
              "      <td>[Why, people, leave, annoying, keyboard, noise...</td>\n",
              "      <td>why people leave annoying keyboard noise phone...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Please would you keep the noise down? We're re...</td>\n",
              "      <td>1</td>\n",
              "      <td>109</td>\n",
              "      <td>[üéª]</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Please would you keep the noise down? We are r...</td>\n",
              "      <td>[Please, would, you, keep, the, noise, down, ?...</td>\n",
              "      <td>[Please, would, keep, noise, We, rehearsing, D...</td>\n",
              "      <td>please would keep noise we rehearse Dartington</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>I come alive when I hear your voice it's a bea...</td>\n",
              "      <td>0</td>\n",
              "      <td>81</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>I come alive when I hear your voice it is a be...</td>\n",
              "      <td>[I, come, alive, when, I, hear, your, voice, i...</td>\n",
              "      <td>[I, come, alive, I, hear, voice, beautiful, so...</td>\n",
              "      <td>I come alive I hear voice beautiful sound beau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>I'm currently in Ripon, the noise of the thund...</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>I am currently in Ripon, the noise of the thun...</td>\n",
              "      <td>[I, am, currently, in, Ripon, ,, the, noise, o...</td>\n",
              "      <td>[I, currently, Ripon, noise, thunder, fantastic]</td>\n",
              "      <td>I currently ripon noise thunder fantastic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>Sitting down the weir and the noise of the wat...</td>\n",
              "      <td>0</td>\n",
              "      <td>94</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Sitting down the weir and the noise of the wat...</td>\n",
              "      <td>[Sitting, down, the, weir, and, the, noise, of...</td>\n",
              "      <td>[Sitting, weir, noise, water, almost, blocks, ...</td>\n",
              "      <td>sit weir noise water almost block thought head</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>The sound of a beer can being cracked open is ...</td>\n",
              "      <td>0</td>\n",
              "      <td>73</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>The sound of a beer can being cracked open is ...</td>\n",
              "      <td>[The, sound, of, a, beer, can, being, cracked,...</td>\n",
              "      <td>[The, sound, beer, cracked, open, greatest, no...</td>\n",
              "      <td>the sound beer crack open great noise earth</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>I feel so lucky that today is about the quiete...</td>\n",
              "      <td>0</td>\n",
              "      <td>152</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>I feel so lucky that today is about the quiete...</td>\n",
              "      <td>[I, feel, so, lucky, that, today, is, about, t...</td>\n",
              "      <td>[I, feel, lucky, today, quietest, ever, Fewer,...</td>\n",
              "      <td>I feel lucky today quiet ever few copter const...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1019 rows √ó 11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f86c5cf-5237-44e4-9c59-3fcab0a4cb8b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2f86c5cf-5237-44e4-9c59-3fcab0a4cb8b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2f86c5cf-5237-44e4-9c59-3fcab0a4cb8b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLlebJJSfQFR"
      },
      "source": [
        "En este paso tambi√©n podr√≠amos generar nuevas caracter√≠sticas para mejorar el funcionamiento del algoritmo. Por ejemplo, podr√≠amos utilizar TextBlob para obtener el sentimiento (tanto subjetividad y polaridad) de cada twitter. Esto se hace de la siguiente forma:\n",
        "\n",
        "\n",
        "```\n",
        "from textblob import TextBlob\n",
        "Textblob(tweet_text).sentiment.subjectivity\n",
        "Textblob(tweet_text).sentiment.polarity\n",
        "```\n",
        "\n",
        "PAra aplicarlo a cada texto de un dataframe habr√≠a que utilizar funciones Lambda."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p7hWR7Eu-MM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g_TfWEycll6"
      },
      "source": [
        "## Vectorizaci√≥n\n",
        "\n",
        "Una vez hemos limpiado y procesado el texto, vamos a extraer caracter√≠sticas utilizando TFIDFVectorizer:\n",
        "- unigramas, bigramas y trigramas\n",
        "- Que el sistema no considere los elementos que salgan en menos del 5% de los documentos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm7g3CEKfuzm"
      },
      "source": [
        "# BoW Features\n",
        "vectorizador = TfidfVectorizer(min_df=0.01, ngram_range = (1,3))\n",
        "vector_data = vectorizador.fit_transform(dataset[\"lematizacion\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-g8b4ZWyAwA",
        "outputId": "a85544e4-c195-4368-840a-366256e02c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1019x181 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 5327 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOyoWLDrdbM0"
      },
      "source": [
        "# Entrenar/testear el clasificador\n",
        "\n",
        "En esta ocasi√≥n, adem√°s de utilizar las caracter√≠sticas de *Bag of Ngramas* generadas con TfidfVectorizer, nos interesa utilizar otro conjunto de caracter√≠sticas que podr√≠an ser de inter√©s para mejorar el rendimiento del clasificador.\n",
        "\n",
        "En este caso, vamos a introducir como ejemplo las variables de sentimiento de emojis que hemos calculado.\n",
        "\n",
        "La forma m√°s sencilla de hacer esto es utilizar la librer√≠a *scipy* y generar una matriz sparse, comprensible por scikit-learn, que contenga tanto las caracter√≠sticas de TFIDF como las calculadas manualmente. \n",
        "\n",
        "En primer lugar, debemos seleccionar el conjunto de variables que queremos considerar en el entrenamiento. PAra ello hacemos uso del selector `dataframe[[\"nombre_columna1\", \"nombre_columna2\"]]`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5przO751gKhT"
      },
      "source": [
        "extra_features = dataset[['sent_emoji_pos','sent_emoji_neg','sent_emoji_neu']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_data.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfn-rmX2OLsQ",
        "outputId": "9500819c-6872-4abc-b0af-72ccd84be78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float64')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrIyNSnkekfC"
      },
      "source": [
        "Utilizamos la librer√≠a scipy (funci√≥n sparse.hstack) para unir las caracter√≠sticas TFIDF (contenidas en ¬¥vector_data¬¥) con las que acabamos de seleccionar (¬¥extra_features¬¥). Esta uni√≥n nos generar√° una matriz X que utilizaremos para hacer el train-test split posteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5-G9GaxCg3e"
      },
      "source": [
        "import scipy as sp\n",
        "# Extraemos las etiquetas y las asignamos a la variable y\n",
        "y = dataset[\"molestia\"].values.astype(np.float32) \n",
        "# Unimos las caracter√≠sticas TFIDF con las caracter√≠sticas previamente seleccionadas\n",
        "# Extraemos los valores (values) de las extra_features, que es un dataframe  \n",
        "X = sp.sparse.hstack((vector_data,extra_features.values),format='csr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VojEshiwfjG2"
      },
      "source": [
        "Tambi√©n vamos a extraer el nombre de las caracter√≠ticas por si quisieramos utilizarlos con posterioridad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2e09VPeft_g"
      },
      "source": [
        "X_columns=list(vectorizador.get_feature_names_out())+extra_features.columns.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgaXlfF9f9Q5"
      },
      "source": [
        "Vamos a dividir nuestros datos en Train y Test, como habitualmente se hace. En este caso probablemente tuvieramos demasiadas caracter√≠sticas (303 caracter√≠sticas para 764 datos nos va a dar problemas de overfitting, as√≠ que en los ejercicios deber√≠as tener esto en cuenta [bajas el n√∫mero de caracter√≠sticas])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlbpqmfXfuj9",
        "outputId": "2c0cdf32-f825-407b-ce68-095682bebf13"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(764, 184)\n",
            "(255, 184)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692437DAeiyy"
      },
      "source": [
        "**Decision de modelo de ML a utilizar**\n",
        "\n",
        "En primer lugar se ha generado una funci√≥n para medir la calidad de varios modelos est√°ndar de forma f√°cil y ver sus resultados. \n",
        "\n",
        "La funci√≥n hace un KFold y evalua diferentes modelos con una m√©trica de evblauaci√≥n:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos las funcionalidades pertinentes de sklearn:\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "import warnings \n",
        "# Definimos la funci√≥n encargada de evaluar los modelos:\n",
        "def model_evaluation(models, score, X, y):\n",
        "      results = []\n",
        "      names = []\n",
        "      #PAra cada modelo\n",
        "      for name, model in models:\n",
        "          warnings.filterwarnings('ignore') \n",
        "          # Generamos un Kfold\n",
        "          KF = KFold(n_splits = 10, shuffle = True, random_state = 98)\n",
        "\n",
        "          # hacemos croos_val\n",
        "          cv_results = cross_val_score(model, X, y, cv = KF, scoring = score, verbose = False)\n",
        "          \n",
        "          # Guardamos los resultados:\n",
        "          results.append(cv_results)\n",
        "          names.append(name)\n",
        "          \n",
        "          # Mostramos los resultados num√©ricamente:\n",
        "          print('Metric: {} , KFold '.format(str(score)))\n",
        "          print(\"%s: %f (%f) \" % (name, cv_results.mean(), cv_results.std()))\n",
        "\n",
        "      return results, names"
      ],
      "metadata": {
        "id": "5m_42mDgO5s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez definida la funci√≥n, podemos definir los modelos con los que hacer la evaluaci√≥n. En este caso hemos incorporado la regresi√≥n log√≠stica y una naive bayes. "
      ],
      "metadata": {
        "id": "R_KYX8MRSzhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los modelos\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Definimos los modelos y generamos una lista con cada uno de ellos:\n",
        "models = [\n",
        "         (\"Logistic\",LogisticRegression(random_state=30)),\n",
        "         (\"GaussianNB\",GaussianNB())\n",
        "]\n",
        "\n",
        "evaluation_score = \"accuracy\"\n",
        "\n",
        "model_evaluation(models,  evaluation_score, X.toarray(), y)   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujjjeM69OU05",
        "outputId": "382b5d30-c1ba-428f-90d1-2b22436fd9b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metric: accuracy , KFold \n",
            "Logistic: 0.785042 (0.047331) \n",
            "Metric: accuracy , KFold \n",
            "GaussianNB: 0.755620 (0.042968) \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([array([0.74509804, 0.83333333, 0.8627451 , 0.83333333, 0.71568627,\n",
              "         0.82352941, 0.78431373, 0.76470588, 0.74509804, 0.74257426]),\n",
              "  array([0.78431373, 0.7254902 , 0.80392157, 0.79411765, 0.64705882,\n",
              "         0.7745098 , 0.76470588, 0.76470588, 0.76470588, 0.73267327])],\n",
              " ['Logistic', 'GaussianNB'])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJu0sOIggTbi"
      },
      "source": [
        "Definimos las variables para hacer una grid_searc:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1OKzEElg44Z"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# define models and parameters\n",
        "model = LogisticRegression()\n",
        "solvers = ['newton-cg', 'liblinear']\n",
        "penalty = ['l2']\n",
        "c_values = [100, 10, 1.0, 0.1, 0.01]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbK9YtNig7Fz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "4c90f118-eb5e-4138-9f7d-0a652c9a1198"
      },
      "source": [
        "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
        "cv = KFold(n_splits=10)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c05ca60f4205>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolvers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1_micro'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'solvers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QONMSvrbghg0"
      },
      "source": [
        "Vamos a entrenar el grid_search para obtener el mejor par√°metro para nuestro conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL41eC6sg96m",
        "outputId": "52ab0d28-6ef2-496a-9979-057cbc0cb4a1"
      },
      "source": [
        "grid_result = grid_search.fit(X_train, y_train)\n",
        "# summarize results\n",
        "print(\"Mejor accuracy: %f usando los par√°metros %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejor accuracy: 0.770899 usando los par√°metros {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha9YcRp-grU7"
      },
      "source": [
        "Entrenamos el modelo con los resultados ofrecidos por la grid_search:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "x-9g5DOMhBeE",
        "outputId": "6603084b-8cb2-4662-da73-c3392902e7c1"
      },
      "source": [
        "from sklearn.model_selection import (KFold, cross_val_score,cross_validate)\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "model=LogisticRegression(C=10, penalty=\"l2\", solver = \"newton-cg\")\n",
        "model.fit(X_train,y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, solver='newton-cg')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=10, solver=&#x27;newton-cg&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=10, solver=&#x27;newton-cg&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLGMMi1Ng0na"
      },
      "source": [
        "Vamos a ver como funciona el modelo haciendo el predict del test y mostrando la matriz de confusi√≥√±n y el classifciation_Report:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h43h7-Upg04e",
        "outputId": "507bffa2-6324-4eff-e419-e69f03e85cb3"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[105  28]\n",
            " [ 34  88]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.76      0.79      0.77       133\n",
            "         1.0       0.76      0.72      0.74       122\n",
            "\n",
            "    accuracy                           0.76       255\n",
            "   macro avg       0.76      0.76      0.76       255\n",
            "weighted avg       0.76      0.76      0.76       255\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jja7IS6vg_dp"
      },
      "source": [
        "Adem√°s, podr√≠amos mostrar el grado de importancia relativa de las variables dle modelo. Aqu√≠ hago el listado, pero lo ideal ser√≠a seleccionar las m√°s importantes dentro del modelo para saber cuales est√°n teniendo m√°s influencia:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb0sU6ANFBUa"
      },
      "source": [
        "# Obtener la importancia de las variables del modelo\n",
        "importance = model.coef_[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VAyddZlhbjh"
      },
      "source": [
        "A continuaci√≥n utilizamos esa variable de importancia de variables, junto a los nombres de las caracter√≠sticas almacenadas anteriormente en X_columns, para listar la importancia de cada una de las variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxj-SgYChb3_"
      },
      "source": [
        "# Mostrar el n√∫mero de la caracter√≠stica, con su nombre, y su score de importancia\n",
        "for i,v in enumerate(importance):\n",
        " print('Feature: %0d, Name: %s , Score: %.5f' % (i,X_columns[i],v))\n",
        "# plot feature importance\n",
        "plt.bar([x for x in range(len(importance))], importance)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}